{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install flwr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFXE2jbSDkt3",
        "outputId": "42781f3b-c783-45cc-d3a8-4e402082a249"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flwr in /usr/local/lib/python3.10/dist-packages (1.5.0)\n",
            "Requirement already satisfied: cryptography<42.0.0,>=41.0.2 in /usr/local/lib/python3.10/dist-packages (from flwr) (41.0.4)\n",
            "Requirement already satisfied: grpcio!=1.52.0,<2.0.0,>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from flwr) (1.59.0)\n",
            "Requirement already satisfied: iterators<0.0.3,>=0.0.2 in /usr/local/lib/python3.10/dist-packages (from flwr) (0.0.2)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from flwr) (1.23.5)\n",
            "Requirement already satisfied: protobuf<4.0.0,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from flwr) (3.20.3)\n",
            "Requirement already satisfied: pycryptodome<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from flwr) (3.19.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography<42.0.0,>=41.0.2->flwr) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography<42.0.0,>=41.0.2->flwr) (2.21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtMXa5pVEtPr",
        "outputId": "c55c1797-bd5f-4729-977e-0d15774d4e43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.2.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.0.1+cu118)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.9.0)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (23.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.12.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (17.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U flwr[\"simulation\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQygb8FCHv9e",
        "outputId": "9432993d-2f80-4b23-c6f0-1e08ea04f544"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flwr[simulation] in /usr/local/lib/python3.10/dist-packages (1.5.0)\n",
            "Requirement already satisfied: cryptography<42.0.0,>=41.0.2 in /usr/local/lib/python3.10/dist-packages (from flwr[simulation]) (41.0.4)\n",
            "Requirement already satisfied: grpcio!=1.52.0,<2.0.0,>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from flwr[simulation]) (1.59.0)\n",
            "Requirement already satisfied: iterators<0.0.3,>=0.0.2 in /usr/local/lib/python3.10/dist-packages (from flwr[simulation]) (0.0.2)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from flwr[simulation]) (1.23.5)\n",
            "Requirement already satisfied: protobuf<4.0.0,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from flwr[simulation]) (3.20.3)\n",
            "Requirement already satisfied: pycryptodome<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from flwr[simulation]) (3.19.0)\n",
            "Requirement already satisfied: pydantic<2.0.0 in /usr/local/lib/python3.10/dist-packages (from flwr[simulation]) (1.10.13)\n",
            "Requirement already satisfied: ray[default]==2.6.3 in /usr/local/lib/python3.10/dist-packages (from flwr[simulation]) (2.6.3)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray[default]==2.6.3->flwr[simulation]) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray[default]==2.6.3->flwr[simulation]) (3.12.4)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray[default]==2.6.3->flwr[simulation]) (4.19.1)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray[default]==2.6.3->flwr[simulation]) (1.0.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray[default]==2.6.3->flwr[simulation]) (23.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray[default]==2.6.3->flwr[simulation]) (6.0.1)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray[default]==2.6.3->flwr[simulation]) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray[default]==2.6.3->flwr[simulation]) (1.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray[default]==2.6.3->flwr[simulation]) (2.31.0)\n",
            "Requirement already satisfied: aiohttp>=3.7 in /usr/local/lib/python3.10/dist-packages (from ray[default]==2.6.3->flwr[simulation]) (3.8.6)\n",
            "Requirement already satisfied: aiohttp-cors in /usr/local/lib/python3.10/dist-packages (from ray[default]==2.6.3->flwr[simulation]) (0.7.0)\n",
            "Requirement already satisfied: colorful in /usr/local/lib/python3.10/dist-packages (from ray[default]==2.6.3->flwr[simulation]) (0.5.5)\n",
            "Requirement already satisfied: py-spy>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ray[default]==2.6.3->flwr[simulation]) (0.3.14)\n",
            "Requirement already satisfied: gpustat>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray[default]==2.6.3->flwr[simulation]) (1.1.1)\n",
            "Requirement already satisfied: opencensus in /usr/local/lib/python3.10/dist-packages (from ray[default]==2.6.3->flwr[simulation]) (0.11.3)\n",
            "Requirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from ray[default]==2.6.3->flwr[simulation]) (0.17.1)\n",
            "Requirement already satisfied: smart-open in /usr/local/lib/python3.10/dist-packages (from ray[default]==2.6.3->flwr[simulation]) (6.4.0)\n",
            "Requirement already satisfied: virtualenv<20.21.1,>=20.0.24 in /usr/local/lib/python3.10/dist-packages (from ray[default]==2.6.3->flwr[simulation]) (20.21.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography<42.0.0,>=41.0.2->flwr[simulation]) (1.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.0.0->flwr[simulation]) (4.5.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default]==2.6.3->flwr[simulation]) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default]==2.6.3->flwr[simulation]) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default]==2.6.3->flwr[simulation]) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default]==2.6.3->flwr[simulation]) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[default]==2.6.3->flwr[simulation]) (1.9.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography<42.0.0,>=41.0.2->flwr[simulation]) (2.21)\n",
            "Requirement already satisfied: nvidia-ml-py>=11.450.129 in /usr/local/lib/python3.10/dist-packages (from gpustat>=1.0.0->ray[default]==2.6.3->flwr[simulation]) (12.535.108)\n",
            "Requirement already satisfied: psutil>=5.6.0 in /usr/local/lib/python3.10/dist-packages (from gpustat>=1.0.0->ray[default]==2.6.3->flwr[simulation]) (5.9.5)\n",
            "Requirement already satisfied: blessed>=1.17.1 in /usr/local/lib/python3.10/dist-packages (from gpustat>=1.0.0->ray[default]==2.6.3->flwr[simulation]) (1.20.0)\n",
            "Requirement already satisfied: distlib<1,>=0.3.6 in /usr/local/lib/python3.10/dist-packages (from virtualenv<20.21.1,>=20.0.24->ray[default]==2.6.3->flwr[simulation]) (0.3.7)\n",
            "Requirement already satisfied: platformdirs<4,>=2.4 in /usr/local/lib/python3.10/dist-packages (from virtualenv<20.21.1,>=20.0.24->ray[default]==2.6.3->flwr[simulation]) (3.11.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[default]==2.6.3->flwr[simulation]) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[default]==2.6.3->flwr[simulation]) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[default]==2.6.3->flwr[simulation]) (0.10.4)\n",
            "Requirement already satisfied: opencensus-context>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from opencensus->ray[default]==2.6.3->flwr[simulation]) (0.1.3)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opencensus->ray[default]==2.6.3->flwr[simulation]) (2.11.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray[default]==2.6.3->flwr[simulation]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray[default]==2.6.3->flwr[simulation]) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray[default]==2.6.3->flwr[simulation]) (2023.7.22)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from blessed>=1.17.1->gpustat>=1.0.0->ray[default]==2.6.3->flwr[simulation]) (0.2.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from blessed>=1.17.1->gpustat>=1.0.0->ray[default]==2.6.3->flwr[simulation]) (1.16.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]==2.6.3->flwr[simulation]) (1.60.0)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]==2.6.3->flwr[simulation]) (2.17.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]==2.6.3->flwr[simulation]) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]==2.6.3->flwr[simulation]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]==2.6.3->flwr[simulation]) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]==2.6.3->flwr[simulation]) (0.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ll3ltiMMsydj",
        "outputId": "77eedd06-298b-4b85-c9fb-2aed0d2faddd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imported modules.\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier, XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score\n",
        "from sklearn.datasets import load_svmlight_file\n",
        "\n",
        "import numpy as np\n",
        "import torch, torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchmetrics import Accuracy, MeanSquaredError\n",
        "from tqdm import trange, tqdm\n",
        "from torchsummary import summary\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "\n",
        "print(\"Imported modules.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGziwqtTtLVu"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier, XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score\n",
        "from sklearn.datasets import load_svmlight_file\n",
        "\n",
        "import numpy as np\n",
        "import torch, torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchmetrics import Accuracy, MeanSquaredError\n",
        "from tqdm import trange, tqdm\n",
        "from torchsummary import summary\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "\n",
        "from collections import OrderedDict\n",
        "from typing import Any, Dict, List, Optional, Tuple, Union\n",
        "from flwr.common import NDArray, NDArrays\n",
        "import flwr as fl\n",
        "from flwr.common.typing import Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5c94SfwvGND"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "def plot_xgbtree(tree: Union[XGBClassifier, XGBRegressor], n_tree: int) -> None:\n",
        "    \"\"\"Visualize the built xgboost tree.\"\"\"\n",
        "    xgb.plot_tree(tree, num_trees=n_tree)\n",
        "    plt.rcParams[\"figure.figsize\"] = [50, 10]\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def construct_tree(\n",
        "    dataset: Dataset, label: NDArray, n_estimators: int, tree_type: str\n",
        ") -> Union[XGBClassifier, XGBRegressor]:\n",
        "    \"\"\"Construct a xgboost tree form tabular dataset.\"\"\"\n",
        "    if tree_type == \"BINARY\":\n",
        "        tree = xgb.XGBClassifier(\n",
        "            objective=\"binary:logistic\",\n",
        "            learning_rate=0.1,\n",
        "            max_depth=8,\n",
        "            n_estimators=n_estimators,\n",
        "            subsample=0.8,\n",
        "            colsample_bylevel=1,\n",
        "            colsample_bynode=1,\n",
        "            colsample_bytree=1,\n",
        "            alpha=5,\n",
        "            gamma=5,\n",
        "            num_parallel_tree=1,\n",
        "            min_child_weight=1,\n",
        "        )\n",
        "\n",
        "    elif tree_type == \"REG\":\n",
        "        tree = xgb.XGBRegressor(\n",
        "            objective=\"reg:squarederror\",\n",
        "            learning_rate=0.1,\n",
        "            max_depth=8,\n",
        "            n_estimators=n_estimators,\n",
        "            subsample=0.8,\n",
        "            colsample_bylevel=1,\n",
        "            colsample_bynode=1,\n",
        "            colsample_bytree=1,\n",
        "            alpha=5,\n",
        "            gamma=5,\n",
        "            num_parallel_tree=1,\n",
        "            min_child_weight=1,\n",
        "        )\n",
        "\n",
        "    tree.fit(dataset, label)\n",
        "    return tree\n",
        "\n",
        "\n",
        "def construct_tree_from_loader(\n",
        "    dataset_loader: DataLoader, n_estimators: int, tree_type: str\n",
        ") -> Union[XGBClassifier, XGBRegressor]:\n",
        "    \"\"\"Construct a xgboost tree form tabular dataset loader.\"\"\"\n",
        "    for dataset in dataset_loader:\n",
        "        data, label = dataset[0], dataset[1]\n",
        "    return construct_tree(data, label, n_estimators, tree_type)\n",
        "\n",
        "\n",
        "def single_tree_prediction(\n",
        "    tree: Union[XGBClassifier, XGBRegressor], n_tree: int, dataset: NDArray\n",
        ") -> Optional[NDArray]:\n",
        "    \"\"\"Extract the prediction result of a single tree in the xgboost tree\n",
        "    ensemble.\"\"\"\n",
        "    # How to access a single tree\n",
        "    # https://github.com/bmreiniger/datascience.stackexchange/blob/master/57905.ipynb\n",
        "    num_t = len(tree.get_booster().get_dump())\n",
        "    if n_tree > num_t:\n",
        "        print(\n",
        "            \"The tree index to be extracted is larger than the total number of trees.\"\n",
        "        )\n",
        "        return None\n",
        "\n",
        "    return tree.predict(  # type: ignore\n",
        "        dataset, iteration_range=(n_tree, n_tree + 1), output_margin=True\n",
        "    )\n",
        "\n",
        "\n",
        "def tree_encoding(  # pylint: disable=R0914\n",
        "    trainloader: DataLoader,\n",
        "    client_trees: Union[\n",
        "        Tuple[XGBClassifier, int],\n",
        "        Tuple[XGBRegressor, int],\n",
        "        List[Union[Tuple[XGBClassifier, int], Tuple[XGBRegressor, int]]],\n",
        "    ],\n",
        "    client_tree_num: int,\n",
        "    client_num: int,\n",
        ") -> Optional[Tuple[NDArray, NDArray]]:\n",
        "    \"\"\"Transform the tabular dataset into prediction results using the\n",
        "    aggregated xgboost tree ensembles from all clients.\"\"\"\n",
        "    if trainloader is None:\n",
        "        return None\n",
        "\n",
        "    for local_dataset in trainloader:\n",
        "        x_train, y_train = local_dataset[0], local_dataset[1]\n",
        "\n",
        "    x_train_enc = np.zeros((x_train.shape[0], client_num * client_tree_num))\n",
        "    x_train_enc = np.array(x_train_enc, copy=True)\n",
        "\n",
        "    temp_trees: Any = None\n",
        "    if isinstance(client_trees, list) is False:\n",
        "        temp_trees = [client_trees[0]] * client_num\n",
        "    elif isinstance(client_trees, list) and len(client_trees) != client_num:\n",
        "        temp_trees = [client_trees[0][0]] * client_num\n",
        "    else:\n",
        "        cids = []\n",
        "        temp_trees = []\n",
        "        for i, _ in enumerate(client_trees):\n",
        "            temp_trees.append(client_trees[i][0])  # type: ignore\n",
        "            cids.append(client_trees[i][1])  # type: ignore\n",
        "        sorted_index = np.argsort(np.asarray(cids))\n",
        "        temp_trees = np.asarray(temp_trees)[sorted_index]\n",
        "\n",
        "    for i, _ in enumerate(temp_trees):\n",
        "        for j in range(client_tree_num):\n",
        "            x_train_enc[:, i * client_tree_num + j] = single_tree_prediction(\n",
        "                temp_trees[i], j, x_train\n",
        "            )\n",
        "\n",
        "    x_train_enc32: Any = np.float32(x_train_enc)\n",
        "    y_train32: Any = np.float32(y_train)\n",
        "\n",
        "    x_train_enc32, y_train32 = torch.from_numpy(\n",
        "        np.expand_dims(x_train_enc32, axis=1)  # type: ignore\n",
        "    ), torch.from_numpy(\n",
        "        np.expand_dims(y_train32, axis=-1)  # type: ignore\n",
        "    )\n",
        "    return x_train_enc32, y_train32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIZcGaG0vpRF"
      },
      "outputs": [],
      "source": [
        "file_path = \"/content/drive/MyDrive/UNSW\"\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzqYJEg6vtrK"
      },
      "outputs": [],
      "source": [
        "# Train and Test data\n",
        "x_train, y_train = pickle.load(open(file_path+\"/final_train.pkl\", 'rb'))\n",
        "x_test, y_test = pickle.load(open(file_path+'/final_test.pkl', 'rb'))\n",
        "\n",
        "# Dictionaries\n",
        "saved_dict = pickle.load(open(file_path+'/saved_dict.pkl', 'rb'))\n",
        "mode_dict = pickle.load(open(file_path+'/mode_dict.pkl', 'rb'))\n",
        "\n",
        "# Standard scaler\n",
        "scaler = pickle.load(open(file_path+'/scaler.pkl', 'rb'))\n",
        "\n",
        "# Onehot encoders\n",
        "ohe_proto = pickle.load(open(file_path+'/ohe_proto.pkl', 'rb'))\n",
        "ohe_service = pickle.load(open(file_path+'/ohe_service.pkl', 'rb'))\n",
        "ohe_state = pickle.load(open(file_path+'/ohe_state.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZG-Shluvv8j"
      },
      "outputs": [],
      "source": [
        "# data cleaning and plots\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "%matplotlib inline\n",
        "\n",
        "from tqdm import tqdm  # Progress bar\n",
        "\n",
        "import pickle as pickle  # To load data int disk\n",
        "from prettytable import PrettyTable  # To print in tabular format\n",
        "\n",
        "# sklearn: data preprocessing\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from scipy.sparse import csr_matrix  # For sparse matrix\n",
        "\n",
        "# sklearn: train model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score, cross_validate, StratifiedKFold\n",
        "from sklearn.metrics import precision_recall_curve, precision_score, recall_score, f1_score, accuracy_score\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score, confusion_matrix, classification_report\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, make_scorer  # Scoring functions\n",
        "from sklearn.metrics import auc, f1_score, roc_curve, roc_auc_score  # Scoring fns\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV  # Cross validation\n",
        "\n",
        "\n",
        "# sklearn classifiers\n",
        "\n",
        "from sklearn import tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "from sklearn import datasets, ensemble, model_selection\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xd2dKtiGwh3C"
      },
      "outputs": [],
      "source": [
        "# Making the train data sparse matrix\n",
        "x_train_csr = csr_matrix(x_train.values)\n",
        "\n",
        "col = x_train.columns\n",
        "\n",
        "# Creating sparse dataframe with x_train sparse matrix\n",
        "x_train = pd.DataFrame.sparse.from_spmatrix(x_train_csr, columns=col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFOpzOCPwlk1"
      },
      "outputs": [],
      "source": [
        "# Loading sparse data\n",
        "x_train, y_train = pickle.load(open(file_path+'/train_sparse.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwFp3MNWwmzK"
      },
      "outputs": [],
      "source": [
        "#------------------------------------------------------------------------------------------\n",
        "# Data Cleaning\n",
        "#------------------------------------------------------------------------------------------\n",
        "def clean_data(data):\n",
        "    '''\n",
        "    Cleans given raw data. Performs various cleaning, removes Null and wrong values.\n",
        "    Check for columns datatype and fix them.\n",
        "    '''\n",
        "    numerical_col = data.select_dtypes(include=np.number).columns  # All the numerical columns list\n",
        "    categorical_col = data.select_dtypes(exclude=np.number).columns  # All the categorical columns list\n",
        "\n",
        "    # Cleaning the data\n",
        "    for col in data.columns:\n",
        "        val = mode_dict[col]  # Mode value of the column in train data\n",
        "        data[col] = data[col].fillna(value=val)\n",
        "        data[col] = data[col].replace(' ', value=val)\n",
        "        data[col] = data[col].apply(lambda x:\"None\" if x==\"-\" else x)\n",
        "\n",
        "        # Fixing binary columns\n",
        "        if col in saved_dict['binary_col']:\n",
        "            data[col] = np.where(data[col]>1, val, data[col])\n",
        "\n",
        "    # Fixing datatype of columns\n",
        "    bad_dtypes = list(set(categorical_col) - set(saved_dict['cat_col']))\n",
        "    for bad_col in bad_dtypes:\n",
        "        data[col] = data[col].astype(float)\n",
        "\n",
        "    return data\n",
        "\n",
        "#------------------------------------------------------------------------------------------\n",
        "# Feature Engineering: Apply log1p\n",
        "#------------------------------------------------------------------------------------------\n",
        "def apply_log1p(data):\n",
        "    '''\n",
        "    Performs FE on the data. Apply log1p on the specified columns create new column and remove those original columns.\n",
        "    '''\n",
        "    for col in saved_dict['log1p_col']:\n",
        "        new_col = col + '_log1p'  # New col name\n",
        "        data[new_col] = data[col].apply(np.log1p)  # Creating new column on transformed data\n",
        "        data.drop(col, axis=1, inplace=True)  # Removing old columns\n",
        "    return data\n",
        "\n",
        "#------------------------------------------------------------------------------------------\n",
        "# Standardizing: Mean centering an d varience scaling\n",
        "#------------------------------------------------------------------------------------------\n",
        "def standardize(data):\n",
        "    '''\n",
        "    Stanardize the given data. Performs mean centering and varience scaling.\n",
        "    Using stanardscaler object trained on train data.\n",
        "    '''\n",
        "    data[saved_dict['num_col']] = scaler.transform(data[saved_dict['num_col']])\n",
        "    return data\n",
        "\n",
        "#------------------------------------------------------------------------------------------\n",
        "# Onehot encoding of categorical columns\n",
        "#------------------------------------------------------------------------------------------\n",
        "def ohencoding(data):\n",
        "    '''\n",
        "    Onehot encoding the categoricla columns.\n",
        "    Add the ohe columns with the data and removes categorical columns.\n",
        "    Using Onehotencoder objects trained on train data.\n",
        "    '''\n",
        "    # Onehot encoding cat col using onehotencoder objects\n",
        "    X = ohe_service.transform(data['service'].values.reshape(-1, 1))\n",
        "    Xm = ohe_proto.transform(data['proto'].values.reshape(-1, 1))\n",
        "    Xmm = ohe_state.transform(data['state'].values.reshape(-1, 1))\n",
        "\n",
        "    # Adding encoding data to original data\n",
        "    data = pd.concat([data,\n",
        "                      pd.DataFrame(Xm.toarray(), columns=['proto_'+i for i in ohe_proto.categories_[0]]),\n",
        "                      pd.DataFrame(X.toarray(), columns=['service_'+i for i in ohe_service.categories_[0]]),\n",
        "                      pd.DataFrame(Xmm.toarray(), columns=['state_'+i for i in ohe_state.categories_[0]])],\n",
        "                      axis=1)\n",
        "\n",
        "    # Removing cat columns\n",
        "    data.drop(['proto', 'service', 'state'], axis=1, inplace=True)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DacQPjY5wnvG"
      },
      "outputs": [],
      "source": [
        "def get_final_data(data, saved_dict=saved_dict, mode_dict=mode_dict):\n",
        "    '''\n",
        "    This functions takes raw input and convert that to model required output.\n",
        "    '''\n",
        "    data.reset_index(drop=True, inplace=True)\n",
        "    data.columns = saved_dict['columns']\n",
        "\n",
        "    data['network_bytes'] = data['dbytes'] + data['sbytes']\n",
        "\n",
        "    dropable_col = saved_dict['to_drop'] + saved_dict['corr_col']\n",
        "    data.drop(columns=dropable_col, inplace=True)\n",
        "\n",
        "    data = clean_data(data)\n",
        "    data = apply_log1p(data)\n",
        "    data = standardize(data)\n",
        "    data = ohencoding(data)\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-_N3g5NwpZz"
      },
      "outputs": [],
      "source": [
        "# Using pipeline to prepare test data\n",
        "x_test = get_final_data(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHmy5dA_wqVS"
      },
      "outputs": [],
      "source": [
        "# Making test data sparse matrix\n",
        "x_test_csr = csr_matrix(x_test.values)\n",
        "col = x_test.columns\n",
        "\n",
        "# Creating x_test sparse dataframe\n",
        "x_test = pd.DataFrame.sparse.from_spmatrix(x_test_csr, columns=col)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8GUXuOm4mQE",
        "outputId": "6a97a504-efa3-4b42-be82-c4281e94c6c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1778032, 197), (1778032,), (762015, 197), (762015,))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-S_SrmZ5rU_",
        "outputId": "80739acb-036d-4599-b8a6-f66b19387cb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0          0\n",
              "1          0\n",
              "2          0\n",
              "3          0\n",
              "4          0\n",
              "          ..\n",
              "1778027    0\n",
              "1778028    0\n",
              "1778029    0\n",
              "1778030    0\n",
              "1778031    0\n",
              "Name: label, Length: 1778032, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Stnhs_yFwy3q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffd9abd5-7624-4c38-b37f-741fbad11fd0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1778032, 198), (762015, 198))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "data_train = x_train\n",
        "data_train[\"label\"]=y_train\n",
        "data_test = x_test\n",
        "data_test[\"label\"]=y_test\n",
        "data_train.shape,data_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZ6vZM4cxf8u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7eba523b-ba5c-4870-a510-cdd3cfcc58ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature dimension of the dataset: 197\n",
            "Size of the trainset: 1778032\n",
            "Size of the testset: 762015\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "class TreeDataset(Dataset):\n",
        "    def __init__(self, data: NDArray, labels: NDArray) -> None:\n",
        "        self.labels = labels\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[int, NDArray]:\n",
        "        label = self.labels[idx]\n",
        "        data = self.data[idx, :]\n",
        "        sample = {0: data, 1: label}\n",
        "        return sample\n",
        "\n",
        "\n",
        "#X_train = np.asarray(data_train.iloc[:,0])\n",
        "X_train = data_train[data_train.columns[:-1].tolist()]\n",
        "#y_train = data_train.iloc[:,1]\n",
        "y_train = data_train[data_train.columns[-1]]\n",
        "#X_test = np.asarray(data_test.iloc[:,0])\n",
        "#y_test = data_test.iloc[:,1]\n",
        "X_test= data_test[data_test.columns[:-1].tolist()]\n",
        "y_test = data_test[data_test.columns[-1]]\n",
        "X_train.flags.writeable = True\n",
        "y_train.flags.writeable = True\n",
        "X_test.flags.writeable = True\n",
        "y_test.flags.writeable = True\n",
        "\n",
        "# If the feature dimensions of the trainset and testset do not agree,\n",
        "# specify n_features in the load_svmlight_file function in the above cell.\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_svmlight_file.html\n",
        "print(\"Feature dimension of the dataset:\", X_train.shape[1])\n",
        "print(\"Size of the trainset:\", X_train.shape[0])\n",
        "print(\"Size of the testset:\", X_test.shape[0])\n",
        "print(X_train.shape[1] == X_test.shape[1])\n",
        "\n",
        "\n",
        "y_train[y_train == -1] = 0\n",
        "y_test[y_test == -1] = 0\n",
        "\n",
        "trainset = TreeDataset(np.array(X_train, copy=True), np.array(y_train, copy=True))\n",
        "testset = TreeDataset(np.array(X_test, copy=True), np.array(y_test, copy=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = data_train[data_train.columns[-1]]\n",
        "y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGb4cHr2_dww",
        "outputId": "ca4cf9de-b563-4d76-ed0e-aa62864a1bde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0          0\n",
              "1          0\n",
              "2          0\n",
              "3          0\n",
              "4          0\n",
              "          ..\n",
              "1778027    0\n",
              "1778028    0\n",
              "1778029    0\n",
              "1778030    0\n",
              "1778031    0\n",
              "Name: label, Length: 1778032, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4AJ6Tn2xka9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65bfdb00-1d46-44d3-dafa-c89073b6c127"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1778032, 197), (762015, 197))"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "X_train.shape,X_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conduct tabular dataset partition for Federated Learning"
      ],
      "metadata": {
        "id": "Iu4If69Q2phO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataloader(\n",
        "    dataset: Dataset, partition: str, batch_size: Union[int, str]\n",
        ") -> DataLoader:\n",
        "    if batch_size == \"whole\":\n",
        "        batch_size = len(dataset)\n",
        "    return DataLoader(\n",
        "        dataset, batch_size=batch_size, pin_memory=True, shuffle=(partition == \"train\")\n",
        "    )\n",
        "\n",
        "\n",
        "# https://github.com/adap/flower\n",
        "def do_fl_partitioning(\n",
        "    trainset: Dataset,\n",
        "    testset: Dataset,\n",
        "    pool_size: int,\n",
        "    batch_size: Union[int, str],\n",
        "    val_ratio: float = 0.0,\n",
        ") -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
        "    # Split training set into `num_clients` partitions to simulate different local datasets\n",
        "    partition_size = len(trainset) // pool_size\n",
        "    lengths = [partition_size] * pool_size\n",
        "    if sum(lengths) != len(trainset):\n",
        "        lengths[-1] = len(trainset) - sum(lengths[0:-1])\n",
        "    datasets = random_split(trainset, lengths, torch.Generator().manual_seed(0))\n",
        "\n",
        "    # Split each partition into train/val and create DataLoader\n",
        "    trainloaders = []\n",
        "    valloaders = []\n",
        "    for ds in datasets:\n",
        "        len_val = int(len(ds) * val_ratio)\n",
        "        len_train = len(ds) - len_val\n",
        "        lengths = [len_train, len_val]\n",
        "        ds_train, ds_val = random_split(ds, lengths, torch.Generator().manual_seed(0))\n",
        "        trainloaders.append(get_dataloader(ds_train, \"train\", batch_size))\n",
        "        if len_val != 0:\n",
        "            valloaders.append(get_dataloader(ds_val, \"val\", batch_size))\n",
        "        else:\n",
        "            valloaders = None\n",
        "    testloader = get_dataloader(testset, \"test\", batch_size)\n",
        "    return trainloaders, valloaders, testloader"
      ],
      "metadata": {
        "id": "4klcwuP52lp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define global variables for Federated XGBoost Learning"
      ],
      "metadata": {
        "id": "bTDeis7r2tSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# The number of clients participated in the federated learning\n",
        "client_num = 5\n",
        "\n",
        "# The number of XGBoost trees in the tree ensemble that will be built for each client\n",
        "client_tree_num = 500 // client_num"
      ],
      "metadata": {
        "id": "fNRIVmRJ2sDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build global XGBoost tree for comparison\n"
      ],
      "metadata": {
        "id": "V9Dpp27z2w9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape,X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoC9kbsA3uNt",
        "outputId": "be4ed043-6951-43fe-ff69-b2518bd6f1dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((762015, 197), (1778032, 197))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbtb4U-J_LB0",
        "outputId": "001a7f25-9df5-4c13-c56f-84cda6aa0a95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0          0\n",
              "1          0\n",
              "2          0\n",
              "3          0\n",
              "4          0\n",
              "          ..\n",
              "1778027    0\n",
              "1778028    0\n",
              "1778029    0\n",
              "1778030    0\n",
              "1778031    0\n",
              "Name: label, Length: 1778032, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "task_type=\"BINARY\"\n",
        "global_tree = construct_tree(X_train, y_train, client_tree_num, task_type)\n",
        "preds_train = global_tree.predict(X_train)\n",
        "preds_test = global_tree.predict(X_test)"
      ],
      "metadata": {
        "id": "vJzppG4c2usv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_train = accuracy_score(y_train, preds_train)\n",
        "result_test = accuracy_score(y_test, preds_test)\n",
        "print(\"Global XGBoost Training Accuracy: %f\" % (result_train))\n",
        "print(\"Global XGBoost Testing Accuracy: %f\" % (result_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxk8pxeb2yeW",
        "outputId": "42057609-6285-48b6-8690-582290fed437"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global XGBoost Training Accuracy: 0.994098\n",
            "Global XGBoost Testing Accuracy: 0.993655\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(global_tree)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbuU0n-B3M-X",
        "outputId": "df1bf430-5e6c-4a30-fd14-21da735a26e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBClassifier(alpha=5, base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
            "              device=None, early_stopping_rounds=None, enable_categorical=False,\n",
            "              eval_metric=None, feature_types=None, gamma=5, grow_policy=None,\n",
            "              importance_type=None, interaction_constraints=None,\n",
            "              learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n",
            "              max_cat_to_onehot=None, max_delta_step=None, max_depth=8,\n",
            "              max_leaves=None, min_child_weight=1, missing=nan,\n",
            "              monotone_constraints=None, multi_strategy=None, n_estimators=100,\n",
            "              n_jobs=None, num_parallel_tree=1, ...)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simulate local XGBoost trees on clients for comparison"
      ],
      "metadata": {
        "id": "eNFMmdT33P-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client_trees_comparison = []\n",
        "trainloaders, _, testloader = do_fl_partitioning(\n",
        "    trainset, testset, pool_size=client_num, batch_size=\"whole\", val_ratio=0.0\n",
        ")\n",
        "\n",
        "for i, trainloader in enumerate(trainloaders):\n",
        "    for local_dataset in trainloader:\n",
        "        local_X_train, local_y_train = local_dataset[0], local_dataset[1]\n",
        "        tree = construct_tree(local_X_train, local_y_train, client_tree_num, task_type)\n",
        "        client_trees_comparison.append(tree)\n",
        "\n",
        "        preds_train = client_trees_comparison[-1].predict(local_X_train)\n",
        "        preds_test = client_trees_comparison[-1].predict(X_test)\n",
        "\n",
        "        if task_type == \"BINARY\":\n",
        "            result_train = accuracy_score(local_y_train, preds_train)\n",
        "            result_test = accuracy_score(y_test, preds_test)\n",
        "            print(\"Local Client %d XGBoost Training Accuracy: %f\" % (i, result_train))\n",
        "            print(\"Local Client %d XGBoost Testing Accuracy: %f\" % (i, result_test))\n",
        "        elif task_type == \"REG\":\n",
        "            result_train = mean_squared_error(local_y_train, preds_train)\n",
        "            result_test = mean_squared_error(y_test, preds_test)\n",
        "            print(\"Local Client %d XGBoost Training MSE: %f\" % (i, result_train))\n",
        "            print(\"Local Client %d XGBoost Testing MSE: %f\" % (i, result_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6r3H5KTt3ObW",
        "outputId": "a9eb4c7d-582b-4434-ea74-ce7d186dd3ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Local Client 0 XGBoost Training Accuracy: 0.993406\n",
            "Local Client 0 XGBoost Testing Accuracy: 0.992396\n",
            "Local Client 1 XGBoost Training Accuracy: 0.993296\n",
            "Local Client 1 XGBoost Testing Accuracy: 0.992491\n",
            "Local Client 2 XGBoost Training Accuracy: 0.993389\n",
            "Local Client 2 XGBoost Testing Accuracy: 0.992554\n",
            "Local Client 3 XGBoost Training Accuracy: 0.993167\n",
            "Local Client 3 XGBoost Testing Accuracy: 0.992348\n",
            "Local Client 4 XGBoost Training Accuracy: 0.993094\n",
            "Local Client 4 XGBoost Testing Accuracy: 0.992459\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Centralized Federated XGBoost\n",
        "Create 1D convolutional neural network on trees prediction results.\n",
        "1D kernel size == client_tree_num\n",
        "Make the learning rate of the tree ensembles learnable."
      ],
      "metadata": {
        "id": "XUxbWNA-3UMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, n_channel: int = 64) -> None:\n",
        "        super(CNN, self).__init__()\n",
        "        n_out = 1\n",
        "        self.task_type = task_type\n",
        "        self.conv1d = nn.Conv1d(\n",
        "            1, n_channel, kernel_size=client_tree_num, stride=client_tree_num, padding=0\n",
        "        )\n",
        "        self.layer_direct = nn.Linear(n_channel * client_num, n_out)\n",
        "        self.ReLU = nn.ReLU()\n",
        "        self.Sigmoid = nn.Sigmoid()\n",
        "        self.Identity = nn.Identity()\n",
        "\n",
        "        # Add weight initialization\n",
        "        for layer in self.modules():\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                nn.init.kaiming_uniform_(\n",
        "                    layer.weight, mode=\"fan_in\", nonlinearity=\"relu\"\n",
        "                )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.ReLU(self.conv1d(x))\n",
        "        x = x.flatten(start_dim=1)\n",
        "        x = self.ReLU(x)\n",
        "        if self.task_type == \"BINARY\":\n",
        "            x = self.Sigmoid(self.layer_direct(x))\n",
        "        elif self.task_type == \"REG\":\n",
        "            x = self.Identity(self.layer_direct(x))\n",
        "        return x\n",
        "\n",
        "    def get_weights(self) -> fl.common.NDArrays:\n",
        "        \"\"\"Get model weights as a list of NumPy ndarrays.\"\"\"\n",
        "        return [\n",
        "            np.array(val.cpu().numpy(), copy=True)\n",
        "            for _, val in self.state_dict().items()\n",
        "        ]\n",
        "\n",
        "    def set_weights(self, weights: fl.common.NDArrays) -> None:\n",
        "        \"\"\"Set model weights from a list of NumPy ndarrays.\"\"\"\n",
        "        layer_dict = {}\n",
        "        for k, v in zip(self.state_dict().keys(), weights):\n",
        "            if v.ndim != 0:\n",
        "                layer_dict[k] = torch.Tensor(np.array(v, copy=True))\n",
        "        state_dict = OrderedDict(layer_dict)\n",
        "        self.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "\n",
        "def train(\n",
        "    task_type: str,\n",
        "    net: CNN,\n",
        "    trainloader: DataLoader,\n",
        "    device: torch.device,\n",
        "    num_iterations: int,\n",
        "    log_progress: bool = True,\n",
        ") -> Tuple[float, float, int]:\n",
        "    # Define loss and optimizer\n",
        "    if task_type == \"BINARY\":\n",
        "        criterion = nn.BCELoss()\n",
        "    elif task_type == \"REG\":\n",
        "        criterion = nn.MSELoss()\n",
        "    # optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-6)\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=0.0001, betas=(0.9, 0.999))\n",
        "\n",
        "    def cycle(iterable):\n",
        "        \"\"\"Repeats the contents of the train loader, in case it gets exhausted in 'num_iterations'.\"\"\"\n",
        "        while True:\n",
        "            for x in iterable:\n",
        "                yield x\n",
        "\n",
        "    # Train the network\n",
        "    net.train()\n",
        "    total_loss, total_result, n_samples = 0.0, 0.0, 0\n",
        "    pbar = (\n",
        "        tqdm(iter(cycle(trainloader)), total=num_iterations, desc=f\"TRAIN\")\n",
        "        if log_progress\n",
        "        else iter(cycle(trainloader))\n",
        "    )\n",
        "\n",
        "    # Unusually, this training is formulated in terms of number of updates/iterations/batches processed\n",
        "    # by the network. This will be helpful later on, when partitioning the data across clients: resulting\n",
        "    # in differences between dataset sizes and hence inconsistent numbers of updates per 'epoch'.\n",
        "    for i, data in zip(range(num_iterations), pbar):\n",
        "        tree_outputs, labels = data[0].to(device), data[1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = net(tree_outputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Collected training loss and accuracy statistics\n",
        "        total_loss += loss.item()\n",
        "        n_samples += labels.size(0)\n",
        "\n",
        "        if task_type == \"BINARY\":\n",
        "            acc = Accuracy(task=\"binary\")(outputs, labels.type(torch.int))\n",
        "            total_result += acc * labels.size(0)\n",
        "        elif task_type == \"REG\":\n",
        "            mse = MeanSquaredError()(outputs, labels.type(torch.int))\n",
        "            total_result += mse * labels.size(0)\n",
        "\n",
        "        if log_progress:\n",
        "            if task_type == \"BINARY\":\n",
        "                pbar.set_postfix(\n",
        "                    {\n",
        "                        \"train_loss\": total_loss / n_samples,\n",
        "                        \"train_acc\": total_result / n_samples,\n",
        "                    }\n",
        "                )\n",
        "            elif task_type == \"REG\":\n",
        "                pbar.set_postfix(\n",
        "                    {\n",
        "                        \"train_loss\": total_loss / n_samples,\n",
        "                        \"train_mse\": total_result / n_samples,\n",
        "                    }\n",
        "                )\n",
        "    if log_progress:\n",
        "        print(\"\\n\")\n",
        "\n",
        "    return total_loss / n_samples, total_result / n_samples, n_samples\n",
        "\n",
        "\n",
        "def test(\n",
        "    task_type: str,\n",
        "    net: CNN,\n",
        "    testloader: DataLoader,\n",
        "    device: torch.device,\n",
        "    log_progress: bool = True,\n",
        ") -> Tuple[float, float, int]:\n",
        "    \"\"\"Evaluates the network on test data.\"\"\"\n",
        "    if task_type == \"BINARY\":\n",
        "        criterion = nn.BCELoss()\n",
        "    elif task_type == \"REG\":\n",
        "        criterion = nn.MSELoss()\n",
        "\n",
        "    total_loss, total_result, n_samples = 0.0, 0.0, 0\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(testloader, desc=\"TEST\") if log_progress else testloader\n",
        "        for data in pbar:\n",
        "            tree_outputs, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = net(tree_outputs)\n",
        "\n",
        "            # Collected testing loss and accuracy statistics\n",
        "            total_loss += criterion(outputs, labels).item()\n",
        "            n_samples += labels.size(0)\n",
        "\n",
        "            if task_type == \"BINARY\":\n",
        "                acc = Accuracy(task=\"binary\")(\n",
        "                    outputs.cpu(), labels.type(torch.int).cpu()\n",
        "                )\n",
        "                total_result += acc * labels.size(0)\n",
        "            elif task_type == \"REG\":\n",
        "                mse = MeanSquaredError()(outputs.cpu(), labels.type(torch.int).cpu())\n",
        "                total_result += mse * labels.size(0)\n",
        "\n",
        "    if log_progress:\n",
        "        print(\"\\n\")\n",
        "\n",
        "    return total_loss / n_samples, total_result / n_samples, n_samples"
      ],
      "metadata": {
        "id": "xH0-wFDs3SWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Flower custom client\n",
        "Import Flower custom client relevant modules"
      ],
      "metadata": {
        "id": "P4bH0a6y3dfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Flower client\n",
        "from flwr.common import (\n",
        "    EvaluateIns,\n",
        "    EvaluateRes,\n",
        "    FitIns,\n",
        "    FitRes,\n",
        "    GetPropertiesIns,\n",
        "    GetPropertiesRes,\n",
        "    GetParametersIns,\n",
        "    GetParametersRes,\n",
        "    Status,\n",
        "    Code,\n",
        "    parameters_to_ndarrays,\n",
        "    ndarrays_to_parameters,\n",
        ")\n"
      ],
      "metadata": {
        "id": "dGuLkSwr3Y0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tree_encoding_loader(\n",
        "    dataloader: DataLoader,\n",
        "    batch_size: int,\n",
        "    client_trees: Union[\n",
        "        Tuple[XGBClassifier, int],\n",
        "        Tuple[XGBRegressor, int],\n",
        "        List[Union[Tuple[XGBClassifier, int], Tuple[XGBRegressor, int]]],\n",
        "    ],\n",
        "    client_tree_num: int,\n",
        "    client_num: int,\n",
        ") -> DataLoader:\n",
        "    encoding = tree_encoding(dataloader, client_trees, client_tree_num, client_num)\n",
        "    if encoding is None:\n",
        "        return None\n",
        "    data, labels = encoding\n",
        "    tree_dataset = TreeDataset(data, labels)\n",
        "    return get_dataloader(tree_dataset, \"tree\", batch_size)\n",
        "\n",
        "\n",
        "class FL_Client(fl.client.Client):\n",
        "    def __init__(\n",
        "        self,\n",
        "        task_type: str,\n",
        "        trainloader: DataLoader,\n",
        "        valloader: DataLoader,\n",
        "        client_tree_num: int,\n",
        "        client_num: int,\n",
        "        cid: str,\n",
        "        log_progress: bool = False,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Creates a client for training `network.Net` on tabular dataset.\n",
        "        \"\"\"\n",
        "        self.task_type = task_type\n",
        "        self.cid = cid\n",
        "        self.tree = construct_tree_from_loader(trainloader, client_tree_num, task_type)\n",
        "        self.trainloader_original = trainloader\n",
        "        self.valloader_original = valloader\n",
        "        self.trainloader = None\n",
        "        self.valloader = None\n",
        "        self.client_tree_num = client_tree_num\n",
        "        self.client_num = client_num\n",
        "        self.properties = {\"tensor_type\": \"numpy.ndarray\"}\n",
        "        self.log_progress = log_progress\n",
        "\n",
        "        # instantiate model\n",
        "        self.net = CNN()\n",
        "\n",
        "        # determine device\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def get_properties(self, ins: GetPropertiesIns) -> GetPropertiesRes:\n",
        "        return GetPropertiesRes(properties=self.properties)\n",
        "\n",
        "    def get_parameters(\n",
        "        self, ins: GetParametersIns\n",
        "    ) -> Tuple[\n",
        "        GetParametersRes, Union[Tuple[XGBClassifier, int], Tuple[XGBRegressor, int]]\n",
        "    ]:\n",
        "        return [\n",
        "            GetParametersRes(\n",
        "                status=Status(Code.OK, \"\"),\n",
        "                parameters=ndarrays_to_parameters(self.net.get_weights()),\n",
        "            ),\n",
        "            (self.tree, int(self.cid)),\n",
        "        ]\n",
        "\n",
        "    def set_parameters(\n",
        "        self,\n",
        "        parameters: Tuple[\n",
        "            Parameters,\n",
        "            Union[\n",
        "                Tuple[XGBClassifier, int],\n",
        "                Tuple[XGBRegressor, int],\n",
        "                List[Union[Tuple[XGBClassifier, int], Tuple[XGBRegressor, int]]],\n",
        "            ],\n",
        "        ],\n",
        "    ) -> Union[\n",
        "        Tuple[XGBClassifier, int],\n",
        "        Tuple[XGBRegressor, int],\n",
        "        List[Union[Tuple[XGBClassifier, int], Tuple[XGBRegressor, int]]],\n",
        "    ]:\n",
        "        self.net.set_weights(parameters_to_ndarrays(parameters[0]))\n",
        "        return parameters[1]\n",
        "\n",
        "    def fit(self, fit_params: FitIns) -> FitRes:\n",
        "        # Process incoming request to train\n",
        "        num_iterations = fit_params.config[\"num_iterations\"]\n",
        "        batch_size = fit_params.config[\"batch_size\"]\n",
        "        aggregated_trees = self.set_parameters(fit_params.parameters)\n",
        "\n",
        "        if type(aggregated_trees) is list:\n",
        "            print(\"Client \" + self.cid + \": recieved\", len(aggregated_trees), \"trees\")\n",
        "        else:\n",
        "            print(\"Client \" + self.cid + \": only had its own tree\")\n",
        "        self.trainloader = tree_encoding_loader(\n",
        "            self.trainloader_original,\n",
        "            batch_size,\n",
        "            aggregated_trees,\n",
        "            self.client_tree_num,\n",
        "            self.client_num,\n",
        "        )\n",
        "        self.valloader = tree_encoding_loader(\n",
        "            self.valloader_original,\n",
        "            batch_size,\n",
        "            aggregated_trees,\n",
        "            self.client_tree_num,\n",
        "            self.client_num,\n",
        "        )\n",
        "\n",
        "        # num_iterations = None special behaviour: train(...) runs for a single epoch, however many updates it may be\n",
        "        num_iterations = num_iterations or len(self.trainloader)\n",
        "\n",
        "        # Train the model\n",
        "        print(f\"Client {self.cid}: training for {num_iterations} iterations/updates\")\n",
        "        self.net.to(self.device)\n",
        "        train_loss, train_result, num_examples = train(\n",
        "            self.task_type,\n",
        "            self.net,\n",
        "            self.trainloader,\n",
        "            device=self.device,\n",
        "            num_iterations=num_iterations,\n",
        "            log_progress=self.log_progress,\n",
        "        )\n",
        "        print(\n",
        "            f\"Client {self.cid}: training round complete, {num_examples} examples processed\"\n",
        "        )\n",
        "\n",
        "        # Return training information: model, number of examples processed and metrics\n",
        "        if self.task_type == \"BINARY\":\n",
        "            return FitRes(\n",
        "                status=Status(Code.OK, \"\"),\n",
        "                parameters=self.get_parameters(fit_params.config),\n",
        "                num_examples=num_examples,\n",
        "                metrics={\"loss\": train_loss, \"accuracy\": train_result},\n",
        "            )\n",
        "        elif self.task_type == \"REG\":\n",
        "            return FitRes(\n",
        "                status=Status(Code.OK, \"\"),\n",
        "                parameters=self.get_parameters(fit_params.config),\n",
        "                num_examples=num_examples,\n",
        "                metrics={\"loss\": train_loss, \"mse\": train_result},\n",
        "            )\n",
        "\n",
        "    def evaluate(self, eval_params: EvaluateIns) -> EvaluateRes:\n",
        "        # Process incoming request to evaluate\n",
        "        self.set_parameters(eval_params.parameters)\n",
        "\n",
        "        # Evaluate the model\n",
        "        self.net.to(self.device)\n",
        "        loss, result, num_examples = test(\n",
        "            self.task_type,\n",
        "            self.net,\n",
        "            self.valloader,\n",
        "            device=self.device,\n",
        "            log_progress=self.log_progress,\n",
        "        )\n",
        "\n",
        "        # Return evaluation information\n",
        "        if self.task_type == \"BINARY\":\n",
        "            print(\n",
        "                f\"Client {self.cid}: evaluation on {num_examples} examples: loss={loss:.4f}, accuracy={result:.4f}\"\n",
        "            )\n",
        "            return EvaluateRes(\n",
        "                status=Status(Code.OK, \"\"),\n",
        "                loss=loss,\n",
        "                num_examples=num_examples,\n",
        "                metrics={\"accuracy\": result},\n",
        "            )\n",
        "        elif self.task_type == \"REG\":\n",
        "            print(\n",
        "                f\"Client {self.cid}: evaluation on {num_examples} examples: loss={loss:.4f}, mse={result:.4f}\"\n",
        "            )\n",
        "            return EvaluateRes(\n",
        "                status=Status(Code.OK, \"\"),\n",
        "                loss=loss,\n",
        "                num_examples=num_examples,\n",
        "                metrics={\"mse\": result},\n",
        "            )"
      ],
      "metadata": {
        "id": "8pzlenLj3ghj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Flower custom server\n",
        "Import Flower custom server relevant modules"
      ],
      "metadata": {
        "id": "IfxAJO5d3nZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Flower server\n",
        "import functools\n",
        "from flwr.server.strategy import FedXgbNnAvg\n",
        "from flwr.server.app import ServerConfig\n",
        "\n",
        "import timeit\n",
        "from logging import DEBUG, INFO\n",
        "from typing import Dict, List, Optional, Tuple, Union\n",
        "\n",
        "from flwr.common import DisconnectRes, Parameters, ReconnectIns, Scalar\n",
        "from flwr.common.logger import log\n",
        "from flwr.common.typing import GetParametersIns\n",
        "from flwr.server.client_manager import ClientManager, SimpleClientManager\n",
        "from flwr.server.client_proxy import ClientProxy\n",
        "from flwr.server.history import History\n",
        "from flwr.server.strategy import Strategy\n",
        "from flwr.server.server import (\n",
        "    reconnect_clients,\n",
        "    reconnect_client,\n",
        "    fit_clients,\n",
        "    fit_client,\n",
        "    _handle_finished_future_after_fit,\n",
        "    evaluate_clients,\n",
        "    evaluate_client,\n",
        "    _handle_finished_future_after_evaluate,\n",
        ")\n",
        "\n",
        "FitResultsAndFailures = Tuple[\n",
        "    List[Tuple[ClientProxy, FitRes]],\n",
        "    List[Union[Tuple[ClientProxy, FitRes], BaseException]],\n",
        "]\n",
        "EvaluateResultsAndFailures = Tuple[\n",
        "    List[Tuple[ClientProxy, EvaluateRes]],\n",
        "    List[Union[Tuple[ClientProxy, EvaluateRes], BaseException]],\n",
        "]"
      ],
      "metadata": {
        "id": "miFaY5ztCSJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FL_Server(fl.server.Server):\n",
        "    \"\"\"Flower server.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, *, client_manager: ClientManager, strategy: Optional[Strategy] = None\n",
        "    ) -> None:\n",
        "        self._client_manager: ClientManager = client_manager\n",
        "        self.parameters: Parameters = Parameters(\n",
        "            tensors=[], tensor_type=\"numpy.ndarray\"\n",
        "        )\n",
        "        self.strategy: Strategy = strategy\n",
        "        self.max_workers: Optional[int] = None\n",
        "\n",
        "    # pylint: disable=too-many-locals\n",
        "    def fit(self, num_rounds: int, timeout: Optional[float]) -> History:\n",
        "        \"\"\"Run federated averaging for a number of rounds.\"\"\"\n",
        "        history = History()\n",
        "\n",
        "        # Initialize parameters\n",
        "        log(INFO, \"Initializing global parameters\")\n",
        "        self.parameters = self._get_initial_parameters(timeout=timeout)\n",
        "\n",
        "        log(INFO, \"Evaluating initial parameters\")\n",
        "        res = self.strategy.evaluate(0, parameters=self.parameters)\n",
        "        if res is not None:\n",
        "            log(\n",
        "                INFO,\n",
        "                \"initial parameters (loss, other metrics): %s, %s\",\n",
        "                res[0],\n",
        "                res[1],\n",
        "            )\n",
        "            history.add_loss_centralized(server_round=0, loss=res[0])\n",
        "            history.add_metrics_centralized(server_round=0, metrics=res[1])\n",
        "\n",
        "        # Run federated learning for num_rounds\n",
        "        log(INFO, \"FL starting\")\n",
        "        start_time = timeit.default_timer()\n",
        "\n",
        "        for current_round in range(1, num_rounds + 1):\n",
        "            # Train model and replace previous global model\n",
        "            res_fit = self.fit_round(server_round=current_round, timeout=timeout)\n",
        "            if res_fit:\n",
        "                parameters_prime, _, _ = res_fit  # fit_metrics_aggregated\n",
        "                if parameters_prime:\n",
        "                    self.parameters = parameters_prime\n",
        "\n",
        "            # Evaluate model using strategy implementation\n",
        "            res_cen = self.strategy.evaluate(current_round, parameters=self.parameters)\n",
        "            if res_cen is not None:\n",
        "                loss_cen, metrics_cen = res_cen\n",
        "                log(\n",
        "                    INFO,\n",
        "                    \"fit progress: (%s, %s, %s, %s)\",\n",
        "                    current_round,\n",
        "                    loss_cen,\n",
        "                    metrics_cen,\n",
        "                    timeit.default_timer() - start_time,\n",
        "                )\n",
        "                history.add_loss_centralized(server_round=current_round, loss=loss_cen)\n",
        "                history.add_metrics_centralized(\n",
        "                    server_round=current_round, metrics=metrics_cen\n",
        "                )\n",
        "\n",
        "            # Evaluate model on a sample of available clients\n",
        "            res_fed = self.evaluate_round(server_round=current_round, timeout=timeout)\n",
        "            if res_fed:\n",
        "                loss_fed, evaluate_metrics_fed, _ = res_fed\n",
        "                if loss_fed:\n",
        "                    history.add_loss_distributed(\n",
        "                        server_round=current_round, loss=loss_fed\n",
        "                    )\n",
        "                    history.add_metrics_distributed(\n",
        "                        server_round=current_round, metrics=evaluate_metrics_fed\n",
        "                    )\n",
        "\n",
        "        # Bookkeeping\n",
        "        end_time = timeit.default_timer()\n",
        "        elapsed = end_time - start_time\n",
        "        log(INFO, \"FL finished in %s\", elapsed)\n",
        "        return history\n",
        "\n",
        "    def evaluate_round(\n",
        "        self,\n",
        "        server_round: int,\n",
        "        timeout: Optional[float],\n",
        "    ) -> Optional[\n",
        "        Tuple[Optional[float], Dict[str, Scalar], EvaluateResultsAndFailures]\n",
        "    ]:\n",
        "        \"\"\"Validate current global model on a number of clients.\"\"\"\n",
        "\n",
        "        # Get clients and their respective instructions from strategy\n",
        "        client_instructions = self.strategy.configure_evaluate(\n",
        "            server_round=server_round,\n",
        "            parameters=self.parameters,\n",
        "            client_manager=self._client_manager,\n",
        "        )\n",
        "        if not client_instructions:\n",
        "            log(INFO, \"evaluate_round %s: no clients selected, cancel\", server_round)\n",
        "            return None\n",
        "        log(\n",
        "            DEBUG,\n",
        "            \"evaluate_round %s: strategy sampled %s clients (out of %s)\",\n",
        "            server_round,\n",
        "            len(client_instructions),\n",
        "            self._client_manager.num_available(),\n",
        "        )\n",
        "\n",
        "        # Collect `evaluate` results from all clients participating in this round\n",
        "        results, failures = evaluate_clients(\n",
        "            client_instructions,\n",
        "            max_workers=self.max_workers,\n",
        "            timeout=timeout,\n",
        "        )\n",
        "        log(\n",
        "            DEBUG,\n",
        "            \"evaluate_round %s received %s results and %s failures\",\n",
        "            server_round,\n",
        "            len(results),\n",
        "            len(failures),\n",
        "        )\n",
        "\n",
        "        # Aggregate the evaluation results\n",
        "        aggregated_result: Tuple[\n",
        "            Optional[float],\n",
        "            Dict[str, Scalar],\n",
        "        ] = self.strategy.aggregate_evaluate(server_round, results, failures)\n",
        "\n",
        "        loss_aggregated, metrics_aggregated = aggregated_result\n",
        "        return loss_aggregated, metrics_aggregated, (results, failures)\n",
        "\n",
        "    def fit_round(\n",
        "        self,\n",
        "        server_round: int,\n",
        "        timeout: Optional[float],\n",
        "    ) -> Optional[\n",
        "        Tuple[\n",
        "            Optional[\n",
        "                Tuple[\n",
        "                    Parameters,\n",
        "                    Union[\n",
        "                        Tuple[XGBClassifier, int],\n",
        "                        Tuple[XGBRegressor, int],\n",
        "                        List[\n",
        "                            Union[Tuple[XGBClassifier, int], Tuple[XGBRegressor, int]]\n",
        "                        ],\n",
        "                    ],\n",
        "                ]\n",
        "            ],\n",
        "            Dict[str, Scalar],\n",
        "            FitResultsAndFailures,\n",
        "        ]\n",
        "    ]:\n",
        "        \"\"\"Perform a single round of federated averaging.\"\"\"\n",
        "\n",
        "        # Get clients and their respective instructions from strategy\n",
        "        client_instructions = self.strategy.configure_fit(\n",
        "            server_round=server_round,\n",
        "            parameters=self.parameters,\n",
        "            client_manager=self._client_manager,\n",
        "        )\n",
        "\n",
        "        if not client_instructions:\n",
        "            log(INFO, \"fit_round %s: no clients selected, cancel\", server_round)\n",
        "            return None\n",
        "        log(\n",
        "            DEBUG,\n",
        "            \"fit_round %s: strategy sampled %s clients (out of %s)\",\n",
        "            server_round,\n",
        "            len(client_instructions),\n",
        "            self._client_manager.num_available(),\n",
        "        )\n",
        "\n",
        "        # Collect `fit` results from all clients participating in this round\n",
        "        results, failures = fit_clients(\n",
        "            client_instructions=client_instructions,\n",
        "            max_workers=self.max_workers,\n",
        "            timeout=timeout,\n",
        "        )\n",
        "\n",
        "        log(\n",
        "            DEBUG,\n",
        "            \"fit_round %s received %s results and %s failures\",\n",
        "            server_round,\n",
        "            len(results),\n",
        "            len(failures),\n",
        "        )\n",
        "\n",
        "        # Aggregate training results\n",
        "        NN_aggregated: Parameters\n",
        "        trees_aggregated: Union[\n",
        "            Tuple[XGBClassifier, int],\n",
        "            Tuple[XGBRegressor, int],\n",
        "            List[Union[Tuple[XGBClassifier, int], Tuple[XGBRegressor, int]]],\n",
        "        ]\n",
        "        metrics_aggregated: Dict[str, Scalar]\n",
        "        aggregated, metrics_aggregated = self.strategy.aggregate_fit(\n",
        "            server_round, results, failures\n",
        "        )\n",
        "        NN_aggregated, trees_aggregated = aggregated[0], aggregated[1]\n",
        "\n",
        "        if type(trees_aggregated) is list:\n",
        "            print(\"Server side aggregated\", len(trees_aggregated), \"trees.\")\n",
        "        else:\n",
        "            print(\"Server side did not aggregate trees.\")\n",
        "\n",
        "        return (\n",
        "            [NN_aggregated, trees_aggregated],\n",
        "            metrics_aggregated,\n",
        "            (results, failures),\n",
        "        )\n",
        "\n",
        "    def _get_initial_parameters(\n",
        "        self, timeout: Optional[float]\n",
        "    ) -> Tuple[Parameters, Union[Tuple[XGBClassifier, int], Tuple[XGBRegressor, int]]]:\n",
        "        \"\"\"Get initial parameters from one of the available clients.\"\"\"\n",
        "\n",
        "        # Server-side parameter initialization\n",
        "        parameters: Optional[Parameters] = self.strategy.initialize_parameters(\n",
        "            client_manager=self._client_manager\n",
        "        )\n",
        "        if parameters is not None:\n",
        "            log(INFO, \"Using initial parameters provided by strategy\")\n",
        "            return parameters\n",
        "\n",
        "        # Get initial parameters from one of the clients\n",
        "        log(INFO, \"Requesting initial parameters from one random client\")\n",
        "        random_client = self._client_manager.sample(1)[0]\n",
        "        ins = GetParametersIns(config={})\n",
        "        get_parameters_res_tree = random_client.get_parameters(ins=ins, timeout=timeout)\n",
        "        parameters = [get_parameters_res_tree[0].parameters, get_parameters_res_tree[1]]\n",
        "        log(INFO, \"Received initial parameters from one random client\")\n",
        "\n",
        "        return parameters"
      ],
      "metadata": {
        "id": "c_-OyuJECV-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create server-side evaluation and experiment"
      ],
      "metadata": {
        "id": "nC2bhxsWCbOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_model_layers(model: nn.Module) -> None:\n",
        "    print(model)\n",
        "    for param_tensor in model.state_dict():\n",
        "        print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
        "\n",
        "\n",
        "def serverside_eval(\n",
        "    server_round: int,\n",
        "    parameters: Tuple[\n",
        "        Parameters,\n",
        "        Union[\n",
        "            Tuple[XGBClassifier, int],\n",
        "            Tuple[XGBRegressor, int],\n",
        "            List[Union[Tuple[XGBClassifier, int], Tuple[XGBRegressor, int]]],\n",
        "        ],\n",
        "    ],\n",
        "    config: Dict[str, Scalar],\n",
        "    task_type: str,\n",
        "    testloader: DataLoader,\n",
        "    batch_size: int,\n",
        "    client_tree_num: int,\n",
        "    client_num: int,\n",
        ") -> Tuple[float, Dict[str, float]]:\n",
        "    \"\"\"An evaluation function for centralized/serverside evaluation over the entire test set.\"\"\"\n",
        "    # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    device = \"cpu\"\n",
        "    model = CNN()\n",
        "    # print_model_layers(model)\n",
        "\n",
        "    model.set_weights(parameters_to_ndarrays(parameters[0]))\n",
        "    model.to(device)\n",
        "\n",
        "    trees_aggregated = parameters[1]\n",
        "    testloader = tree_encoding_loader(\n",
        "        testloader, batch_size, trees_aggregated, client_tree_num, client_num\n",
        "    )\n",
        "    loss, result, _ = test(\n",
        "        task_type, model, testloader, device=device, log_progress=False\n",
        "    )\n",
        "\n",
        "    if task_type == \"BINARY\":\n",
        "        print(\n",
        "            f\"Evaluation on the server: test_loss={loss:.4f}, test_accuracy={result:.4f}\"\n",
        "        )\n",
        "        return loss, {\"accuracy\": result}\n",
        "    elif task_type == \"REG\":\n",
        "        print(f\"Evaluation on the server: test_loss={loss:.4f}, test_mse={result:.4f}\")\n",
        "        return loss, {\"mse\": result}\n",
        "\n",
        "\n",
        "def start_experiment(\n",
        "    task_type: str,\n",
        "    trainset: Dataset,\n",
        "    testset: Dataset,\n",
        "    num_rounds: int = 5,\n",
        "    client_tree_num: int = 50,\n",
        "    client_pool_size: int = 5,\n",
        "    num_iterations: int = 100,\n",
        "    fraction_fit: float = 1.0,\n",
        "    min_fit_clients: int = 2,\n",
        "    batch_size: int = 32,\n",
        "    val_ratio: float = 0.1,\n",
        ") -> History:\n",
        "    client_resources = {\"num_cpus\": 0.5}  # 2 clients per CPU\n",
        "\n",
        "    # Partition the dataset into subsets reserved for each client.\n",
        "    # - 'val_ratio' controls the proportion of the (local) client reserved as a local test set\n",
        "    # (good for testing how the final model performs on the client's local unseen data)\n",
        "    trainloaders, valloaders, testloader = do_fl_partitioning(\n",
        "        trainset,\n",
        "        testset,\n",
        "        batch_size=\"whole\",\n",
        "        pool_size=client_pool_size,\n",
        "        val_ratio=val_ratio,\n",
        "    )\n",
        "    print(\n",
        "        f\"Data partitioned across {client_pool_size} clients\"\n",
        "        f\" and {val_ratio} of local dataset reserved for validation.\"\n",
        "    )\n",
        "\n",
        "    # Configure the strategy\n",
        "    def fit_config(server_round: int) -> Dict[str, Scalar]:\n",
        "        print(f\"Configuring round {server_round}\")\n",
        "        return {\n",
        "            \"num_iterations\": num_iterations,\n",
        "            \"batch_size\": batch_size,\n",
        "        }\n",
        "\n",
        "    # FedXgbNnAvg\n",
        "    strategy = FedXgbNnAvg(\n",
        "        fraction_fit=fraction_fit,\n",
        "        fraction_evaluate=fraction_fit if val_ratio > 0.0 else 0.0,\n",
        "        min_fit_clients=min_fit_clients,\n",
        "        min_evaluate_clients=min_fit_clients,\n",
        "        min_available_clients=client_pool_size,  # all clients should be available\n",
        "        on_fit_config_fn=fit_config,\n",
        "        on_evaluate_config_fn=(lambda r: {\"batch_size\": batch_size}),\n",
        "        evaluate_fn=functools.partial(\n",
        "            serverside_eval,\n",
        "            task_type=task_type,\n",
        "            testloader=testloader,\n",
        "            batch_size=batch_size,\n",
        "            client_tree_num=client_tree_num,\n",
        "            client_num=client_num,\n",
        "        ),\n",
        "        accept_failures=False,\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"FL experiment configured for {num_rounds} rounds with {client_pool_size} client in the pool.\"\n",
        "    )\n",
        "    print(\n",
        "        f\"FL round will proceed with {fraction_fit * 100}% of clients sampled, at least {min_fit_clients}.\"\n",
        "    )\n",
        "\n",
        "    def client_fn(cid: str) -> fl.client.Client:\n",
        "        \"\"\"Creates a federated learning client\"\"\"\n",
        "        if val_ratio > 0.0 and val_ratio <= 1.0:\n",
        "            return FL_Client(\n",
        "                task_type,\n",
        "                trainloaders[int(cid)],\n",
        "                valloaders[int(cid)],\n",
        "                client_tree_num,\n",
        "                client_pool_size,\n",
        "                cid,\n",
        "                log_progress=False,\n",
        "            )\n",
        "        else:\n",
        "            return FL_Client(\n",
        "                task_type,\n",
        "                trainloaders[int(cid)],\n",
        "                None,\n",
        "                client_tree_num,\n",
        "                client_pool_size,\n",
        "                cid,\n",
        "                log_progress=False,\n",
        "            )\n",
        "\n",
        "    # Start the simulation\n",
        "    history = fl.simulation.start_simulation(\n",
        "        client_fn=client_fn,\n",
        "        server=FL_Server(client_manager=SimpleClientManager(), strategy=strategy),\n",
        "        num_clients=client_pool_size,\n",
        "        client_resources=client_resources,\n",
        "        config=ServerConfig(num_rounds=num_rounds),\n",
        "        strategy=strategy,\n",
        "    )\n",
        "\n",
        "    print(history)\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "MuVWUGVW3jww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_experiment(\n",
        "    task_type=task_type,\n",
        "    trainset=trainset,\n",
        "    testset=testset,\n",
        "    num_rounds=20,\n",
        "    client_tree_num=client_tree_num,\n",
        "    client_pool_size=client_num,\n",
        "    num_iterations=100,\n",
        "    batch_size=64,\n",
        "    fraction_fit=1.0,\n",
        "    min_fit_clients=1,\n",
        "    val_ratio=0.0,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xRGcsDMCCiZ",
        "outputId": "9d382b4c-36cc-43dd-980b-c85a6f850b8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING flwr 2023-10-16 15:42:31,650 | app.py:210 | Both server and strategy were provided, ignoring strategy\n",
            "WARNING:flwr:Both server and strategy were provided, ignoring strategy\n",
            "INFO flwr 2023-10-16 15:42:31,659 | app.py:175 | Starting Flower simulation, config: ServerConfig(num_rounds=20, round_timeout=None)\n",
            "INFO:flwr:Starting Flower simulation, config: ServerConfig(num_rounds=20, round_timeout=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data partitioned across 5 clients and 0.0 of local dataset reserved for validation.\n",
            "FL experiment configured for 20 rounds with 5 client in the pool.\n",
            "FL round will proceed with 100.0% of clients sampled, at least 1.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-10-16 15:42:41,795\tINFO worker.py:1621 -- Started a local Ray instance.\n",
            "INFO flwr 2023-10-16 15:42:44,309 | app.py:210 | Flower VCE: Ray initialized with resources: {'CPU': 2.0, 'node:__internal_head__': 1.0, 'node:172.28.0.12': 1.0, 'object_store_memory': 3908584243.0, 'memory': 7817168487.0}\n",
            "INFO:flwr:Flower VCE: Ray initialized with resources: {'CPU': 2.0, 'node:__internal_head__': 1.0, 'node:172.28.0.12': 1.0, 'object_store_memory': 3908584243.0, 'memory': 7817168487.0}\n",
            "INFO flwr 2023-10-16 15:42:44,319 | app.py:224 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 0.5}\n",
            "INFO:flwr:Flower VCE: Resources for each Virtual Client: {'num_cpus': 0.5}\n",
            "INFO flwr 2023-10-16 15:42:44,388 | app.py:270 | Flower VCE: Creating VirtualClientEngineActorPool with 4 actors\n",
            "INFO:flwr:Flower VCE: Creating VirtualClientEngineActorPool with 4 actors\n",
            "INFO flwr 2023-10-16 15:42:44,392 | <ipython-input-34-568c4ad0345a>:20 | Initializing global parameters\n",
            "INFO:flwr:Initializing global parameters\n",
            "INFO flwr 2023-10-16 15:42:44,394 | <ipython-input-34-568c4ad0345a>:226 | Requesting initial parameters from one random client\n",
            "INFO:flwr:Requesting initial parameters from one random client\n",
            "ERROR flwr 2023-10-16 15:43:05,388 | ray_client_proxy.py:147 | Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 140, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 402, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 288, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 5267d1715c8f9d7e8e1fc70519da7eb07e4532eee7da2dc734b4cdd3) where the task (task ID: fffffffffffffffff1ef031bc7e1aaf02b1c643101000000, name=DefaultActor.__init__, pid=10084, memory used=0.16GB) was running was 12.05GB / 12.68GB (0.950264), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 30db98f89171119b3a7689eee87e4b4b95e641e912ac8c75ebbe587a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-30db98f89171119b3a7689eee87e4b4b95e641e912ac8c75ebbe587a*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "7691\t10.00\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-4b9ea735-0ff0...\n",
            "8025\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:18f302a5d4ef87d6b...\n",
            "10084\t0.16\tray::IDLE\n",
            "10068\t0.16\tray::IDLE\n",
            "10004\t0.16\tray::IDLE\n",
            "10003\t0.16\tray::IDLE\n",
            "97\t0.11\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4027\t0.11\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "9918\t0.07\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "9833\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR:flwr:Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 140, in _submit_job\n",
            "    res = self.actor_pool.get_client_result(self.cid, timeout)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 402, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 288, in _fetch_future_result\n",
            "    res_cid, res = ray.get(future)  # type: (str, ClientRes)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2526, in get\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 5267d1715c8f9d7e8e1fc70519da7eb07e4532eee7da2dc734b4cdd3) where the task (task ID: fffffffffffffffff1ef031bc7e1aaf02b1c643101000000, name=DefaultActor.__init__, pid=10084, memory used=0.16GB) was running was 12.05GB / 12.68GB (0.950264), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 30db98f89171119b3a7689eee87e4b4b95e641e912ac8c75ebbe587a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-30db98f89171119b3a7689eee87e4b4b95e641e912ac8c75ebbe587a*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "7691\t10.00\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-4b9ea735-0ff0...\n",
            "8025\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:18f302a5d4ef87d6b...\n",
            "10084\t0.16\tray::IDLE\n",
            "10068\t0.16\tray::IDLE\n",
            "10004\t0.16\tray::IDLE\n",
            "10003\t0.16\tray::IDLE\n",
            "97\t0.11\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4027\t0.11\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "9918\t0.07\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "9833\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "ERROR flwr 2023-10-16 15:43:05,409 | ray_client_proxy.py:148 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 5267d1715c8f9d7e8e1fc70519da7eb07e4532eee7da2dc734b4cdd3) where the task (task ID: fffffffffffffffff1ef031bc7e1aaf02b1c643101000000, name=DefaultActor.__init__, pid=10084, memory used=0.16GB) was running was 12.05GB / 12.68GB (0.950264), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 30db98f89171119b3a7689eee87e4b4b95e641e912ac8c75ebbe587a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-30db98f89171119b3a7689eee87e4b4b95e641e912ac8c75ebbe587a*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "7691\t10.00\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-4b9ea735-0ff0...\n",
            "8025\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:18f302a5d4ef87d6b...\n",
            "10084\t0.16\tray::IDLE\n",
            "10068\t0.16\tray::IDLE\n",
            "10004\t0.16\tray::IDLE\n",
            "10003\t0.16\tray::IDLE\n",
            "97\t0.11\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4027\t0.11\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "9918\t0.07\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "9833\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 5267d1715c8f9d7e8e1fc70519da7eb07e4532eee7da2dc734b4cdd3) where the task (task ID: fffffffffffffffff1ef031bc7e1aaf02b1c643101000000, name=DefaultActor.__init__, pid=10084, memory used=0.16GB) was running was 12.05GB / 12.68GB (0.950264), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 30db98f89171119b3a7689eee87e4b4b95e641e912ac8c75ebbe587a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-30db98f89171119b3a7689eee87e4b4b95e641e912ac8c75ebbe587a*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "7691\t10.00\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-4b9ea735-0ff0...\n",
            "8025\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:18f302a5d4ef87d6b...\n",
            "10084\t0.16\tray::IDLE\n",
            "10068\t0.16\tray::IDLE\n",
            "10004\t0.16\tray::IDLE\n",
            "10003\t0.16\tray::IDLE\n",
            "97\t0.11\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4027\t0.11\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "9918\t0.07\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "9833\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2023-10-16 15:43:05,423 | app.py:294 | Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 5267d1715c8f9d7e8e1fc70519da7eb07e4532eee7da2dc734b4cdd3) where the task (task ID: fffffffffffffffff1ef031bc7e1aaf02b1c643101000000, name=DefaultActor.__init__, pid=10084, memory used=0.16GB) was running was 12.05GB / 12.68GB (0.950264), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 30db98f89171119b3a7689eee87e4b4b95e641e912ac8c75ebbe587a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-30db98f89171119b3a7689eee87e4b4b95e641e912ac8c75ebbe587a*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "7691\t10.00\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-4b9ea735-0ff0...\n",
            "8025\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:18f302a5d4ef87d6b...\n",
            "10084\t0.16\tray::IDLE\n",
            "10068\t0.16\tray::IDLE\n",
            "10004\t0.16\tray::IDLE\n",
            "10003\t0.16\tray::IDLE\n",
            "97\t0.11\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4027\t0.11\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "9918\t0.07\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "9833\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR:flwr:Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: 5267d1715c8f9d7e8e1fc70519da7eb07e4532eee7da2dc734b4cdd3) where the task (task ID: fffffffffffffffff1ef031bc7e1aaf02b1c643101000000, name=DefaultActor.__init__, pid=10084, memory used=0.16GB) was running was 12.05GB / 12.68GB (0.950264), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 30db98f89171119b3a7689eee87e4b4b95e641e912ac8c75ebbe587a) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-30db98f89171119b3a7689eee87e4b4b95e641e912ac8c75ebbe587a*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "7691\t10.00\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-4b9ea735-0ff0...\n",
            "8025\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:18f302a5d4ef87d6b...\n",
            "10084\t0.16\tray::IDLE\n",
            "10068\t0.16\tray::IDLE\n",
            "10004\t0.16\tray::IDLE\n",
            "10003\t0.16\tray::IDLE\n",
            "97\t0.11\t/usr/bin/python3 /usr/local/bin/jupyter-notebook --debug --transport=\"ipc\" --ip=172.28.0.12 --Notebo...\n",
            "4027\t0.11\t/opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max...\n",
            "9918\t0.07\t/usr/bin/python3 -u /usr/local/lib/python3.10/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "9833\t0.06\t/usr/bin/python3 /usr/local/lib/python3.10/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "ERROR flwr 2023-10-16 15:43:05,441 | app.py:295 | Your simulation crashed :(. This could be because of several reasons.The most common are: \n",
            "\t > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.\n",
            "\t > All the actors in your pool crashed. This could be because: \n",
            "\t\t - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 0.5} is not enough for your workload). Use fewer concurrent actors. \n",
            "\t\t - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 0.5}.\n",
            "ERROR:flwr:Your simulation crashed :(. This could be because of several reasons.The most common are: \n",
            "\t > Your system couldn't fit a single VirtualClient: try lowering `client_resources`.\n",
            "\t > All the actors in your pool crashed. This could be because: \n",
            "\t\t - You clients hit an out-of-memory (OOM) error and actors couldn't recover from it. Try launching your simulation with more generous `client_resources` setting (i.e. it seems {'num_cpus': 0.5} is not enough for your workload). Use fewer concurrent actors. \n",
            "\t\t - You were running a multi-node simulation and all worker nodes disconnected. The head node might still be alive but cannot accommodate any actor with resources: {'num_cpus': 0.5}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lUdWe5W3HUlR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}